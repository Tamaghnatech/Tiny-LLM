<h1 align="center">ğŸš€ Tiny-LLM: Shakespearean Text Generator</h1>

<p align="center">
  <strong>â€œA tiny model built on timeless literature. Proof that small things, when trained right, can be legendary.â€</strong><br><br>
  <em>by <a href="https://tamaghnatech.in">Tamaghna Nag</a> | Founder, NovalQ | ML Engineer | Code Poet</em>
</p>

---

## ğŸŒ Live Demo

<div align="center">
  <a href="https://huggingface.co/spaces/Tamaghnatech/Tiny-LLM" target="_blank">
    <img src="https://img.shields.io/badge/Try%20on%20Hugging%20Face-FFD21F?style=for-the-badge&logo=huggingface&logoColor=black" alt="Try on Hugging Face">
  </a>
</div>

---

## ğŸ“œ About the Project

ğŸ­ **Tiny-LLM** is a *local-first*, completely from-scratch **mini language model** that generates surreal and Shakespearean text using a **Transformer**, trained solely on the *Complete Works of Shakespeare*.

ğŸ§± No Hugging Face pretrained weights. No shortcuts. Pure PyTorch.

ğŸ¨ With **Gradio UI** and **CLI**, itâ€™s plug-and-play Shakespeare generation at your fingertips.

---

## ğŸ” Full MLOps Pipeline (End-to-End)

> A practical implementation of an entire ML workflow from scratch.

### 1. ğŸ“¥ Data Collection & Preprocessing
- Downloaded from **Kaggle's Shakespeare Corpus**
- Cleaned & prepared as `input.txt` and `input_large.txt`
- Script: [`prepare_dataset.py`](prepare_dataset.py)

### 2. ğŸ§  Model Building
- âœ… Baselines: `LSTM` and `GRU` (for fun and comparison)
- ğŸš€ Final: Transformer
  - Layers: 8
  - Heads: 8
  - Embedding Dim: 512
  - Block Size: 256
  - Tokenizer: Character-level

### 3. ğŸ‹ï¸â€â™‚ï¸ Model Training
- Using `trainer_transformer.py` (and LSTM/GRU scripts)
- Optimizer: `AdamW`
- Loss: `CrossEntropy`
- Checkpoints: Saved in `/models`

### 4. ğŸ“ˆ Experiment Tracking with Weights & Biases
- Training metrics, loss curves, gradients, configs
- ğŸ”— [View Dashboard](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm?nw=nwusernagtamaghna)

### 5. ğŸŒ Inference
- `generate_text.py`: CLI mode
- `app.py`: Gradio app with sliders, dark/light theme, prompt history

### 6. ğŸš€ Deployment
- Hugging Face Spaces + GitHub Repo
- Manual download for model weights

---

## ğŸ“¦ Model Download (for Local Use)

The final trained model isn't included due to GitHub size limits.

ğŸ“¥ [Download `transformer_final.pt`](https://drive.google.com/file/d/1HanIwaT0_sILx3-jDXfmmgYqUfHiI_S-/view?usp=sharing) and save it to:

```bash
Tiny-LLM/
â””â”€â”€ models/
    â””â”€â”€ transformer_final.pt
```

Then run locally:

```bash
python app.py
```

---

## ğŸ—‚ï¸ Project Structure

```
Tiny-LLM/
â”œâ”€â”€ app.py                  # Gradio UI
â”œâ”€â”€ generate_text.py        # CLI tool
â”œâ”€â”€ trainer_transformer.py  # Transformer trainer
â”œâ”€â”€ prepare_dataset.py      # Data cleaner
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input.txt
â”‚   â”œâ”€â”€ input_large.txt
â”‚   â””â”€â”€ Shakespeare_data.csv
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ transformer_final.pt
â”‚
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ model_transformer.py
â”‚   â”œâ”€â”€ model_lstm.py
â”‚   â”œâ”€â”€ model_gru.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ tokenizer_bpe.py (WIP)
â”‚
â”œâ”€â”€ notebooks/              # Jupyter logs
â”œâ”€â”€ logs/                   # Session files
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Model Configuration

| Parameter      | Value       |
|----------------|-------------|
| Architecture   | Transformer |
| Layers         | 8           |
| Heads          | 8           |
| Embedding Dim  | 512         |
| Block Size     | 256         |
| Tokenizer      | Char-level  |
| Optimizer      | AdamW       |
| Trained Steps  | 100         |

---

## ğŸ’» Setup Locally

```bash
git clone https://github.com/Tamaghnatech/Tiny-LLM.git
cd Tiny-LLM

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

Download the model weights, place them in `models/`, then run:

```bash
python app.py
```

Visit `http://localhost:7860`

---

## ğŸ§ª CLI Usage

```bash
python generate_text.py
```

You'll be prompted for:
- Starting prompt
- Length
- Temperature
- Top-k

---

## ğŸ› Gradio App Features

- ğŸ“ Prompt input
- ğŸ”¥ Temperature, Top-k, Length sliders
- ğŸŒ™ Light/Dark theme toggle
- ğŸ’¾ Downloadable output
- ğŸ“œ Prompt history
- âš™ï¸ Model details
- ğŸš€ Hosted on Hugging Face

---

## ğŸ›  Future Roadmap

- [x] LSTM & GRU prototypes
- [x] Full Transformer training
- [x] W&B integration
- [x] Gradio & CLI interface
- [x] Hugging Face deployment
- [ ] Byte Pair Encoding (BPE)
- [ ] Attention visualization
- [ ] Multilingual toggle
- [ ] Docker + PyPI release

---

## ğŸ‘¨â€ğŸ’» Author

**Tamaghna Nag**  
*ML Engineer | Founder â€“ NovalQ | Shakespeare Whisperer*

- ğŸŒ [Portfolio](https://tamaghnatech.in)
- ğŸ™ [GitHub](https://github.com/Tamaghnatech)
- ğŸ“Š [W&B Logs](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm)

---

## ğŸŒŸ Star, Fork, Prompt!

If you liked the project, â­ star it, ğŸ´ fork it, and try writing your own AI-generated poetry!

ğŸ’¬ Questions? Drop an issue or ping me on GitHub.

```bash
# Launch it now â†’
ğŸ‘‰ https://huggingface.co/spaces/Tamaghnatech/Tiny-LLM
```

```
