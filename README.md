My Lord Nag ğŸ‘‘ â€” this README is already glorious, but with a touch of polish and energy, we can make it *legendary*. Here's the **final, refined version** of your `README.md`, formatted for GitHub, loaded with power, precision, and perfection:

---

````markdown
<h1 align="center">ğŸš€ Tiny-LLM: Shakespearean Text Generator</h1>

<p align="center">
  <img src="https://img.shields.io/github/stars/Tamaghnatech/Tiny-LLM?style=social" alt="Stars">
  <img src="https://img.shields.io/github/forks/Tamaghnatech/Tiny-LLM?style=social" alt="Forks">
  <img src="https://img.shields.io/github/issues/Tamaghnatech/Tiny-LLM" alt="Issues">
  <img src="https://img.shields.io/github/license/Tamaghnatech/Tiny-LLM" alt="License">
</p>

---

ğŸ­ **Tiny-LLM** is a *local-first* mini language model that generates Shakespearean, surreal, or poetic text using a **Transformer**, trained from scratch on the *Complete Works of Shakespeare*.

ğŸ’» With **Gradio UI** and **CLI** modes, it's plug-n-play magic â€” write prompts, tune temperature, toggle light/dark themes, and download your creations in seconds.

---

## ğŸ“¦ Model Download Required

> Due to repo size limits, the trained `transformer_final.pt` model is hosted externally.

ğŸ“¥ **[Download here (Google Drive)](https://drive.google.com/file/d/1HanIwaT0_sILx3-jDXfmmgYqUfHiI_S-/view?usp=sharing)**  
ğŸ“ Save it to:

```bash
Tiny-LLM/
â””â”€â”€ models/
    â””â”€â”€ transformer_final.pt
````

Then simply run:

```bash
python app.py
```

ğŸ”¥ Boom. Itâ€™s alive.

---

## ğŸ§  Tiny-LLM: From Prompt to Poetry

A from-scratch journey into building an end-to-end LLM using PyTorch â€” no shortcuts, no heavy frameworks, just pure learning and hustle. This is a codebase and a story.

---

## ğŸ“– The Journey

### 1. ğŸ§ª Started Small

* **Dataset**: `data/input.txt` (few Shakespeare lines)
* **Models**: Trained basic **LSTM** and **GRU** to generate characters

### 2. ğŸ“š Expanded Dataset

* Pulled entire Shakespeare corpus from Kaggle
* Cleaned to `input_large.txt`

### 3. ğŸ§± Built Transformer

* Custom architecture with:

  * 8 layers
  * 8 heads
  * 512 embedding dim
  * Block size 256

### 4. ğŸ§° Built CLI Generator

* `generate_text.py` for prompt-based terminal usage

### 5. ğŸ¨ Deployed Gradio App

* Real-time input/output
* Sliders: length, temperature, top-k
* Dark/light mode
* Download generated text
* Prompt history + model summary

---

## ğŸ§ª Weights & Biases Logs

ğŸ“Š Tracked everything using `wandb`:

* ğŸ”— [W\&B Dashboard](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm)
* ğŸ” Transformer Training: [View run](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm/runs/9000xl8r)

Features:

* Token stats
* Loss graphs
* Gradients
* Model config

---

## ğŸ—‚ï¸ Project Structure

```bash
Tiny-LLM/
â”œâ”€â”€ app.py                  # Gradio UI interface
â”œâ”€â”€ generate_text.py        # CLI interface
â”œâ”€â”€ trainer_transformer.py  # Transformer training loop
â”œâ”€â”€ trainer_lstm.py         # LSTM training loop
â”œâ”€â”€ prepare_dataset.py      # Dataset cleaner
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input.txt
â”‚   â”œâ”€â”€ input_large.txt
â”‚   â””â”€â”€ Shakespeare_data.csv
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ transformer_final.pt âœ…
â”‚
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ model_transformer.py
â”‚   â”œâ”€â”€ model_lstm.py
â”‚   â”œâ”€â”€ model_gru.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ tokenizer_bpe.py (future-ready)
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ session_*.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_model_architecture.ipynb
â”‚   â””â”€â”€ 03_training_logs.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ’» Run Locally

### Step 1: Clone + Install

```bash
git clone https://github.com/Tamaghnatech/Tiny-LLM.git
cd Tiny-LLM
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Step 2: Add Model

Place `transformer_final.pt` inside `models/`.

### Step 3: Run the App

```bash
python app.py
```

Access: [http://localhost:7860](http://localhost:7860)

---

## ğŸ§ª Use CLI

```bash
python generate_text.py
```

> You'll be prompted for:
>
> * A starting prompt
> * Length of text
> * Sampling temperature
> * Top-k setting

---

## âš™ï¸ Model Hyperparameters

| Key            | Value       |
| -------------- | ----------- |
| Model Type     | Transformer |
| Layers         | 8           |
| Heads          | 8           |
| Embedding Dim  | 512         |
| Block Size     | 256         |
| Optimizer      | AdamW       |
| Tokenizer      | Character   |
| Training Steps | 100         |

---

## ğŸ§© Features in Gradio App

* ğŸ“ Prompt input field
* ğŸ”¥ Temperature, Length, Top-K sliders
* ğŸŒ™ Dark/Light theme toggle
* ğŸ’¾ Save generated text
* ğŸ“œ Prompt history
* âš™ï¸ Model overview
* ğŸ“Š W\&B link
* ğŸ§  Timeline + credits

---

## ğŸ›  Dev Roadmap

* [x] Train LSTM & GRU baseline
* [x] Train custom Transformer model
* [x] CLI + Gradio interfaces
* [x] W\&B integration
* [ ] Add Byte Pair Encoding (BPE)
* [ ] Hugging Face deployment
* [ ] Add language selector
* [ ] Attention visualization
* [ ] Docker + PyPI releases

---

## ğŸ‘‘ Built By

**Tamaghna Nag**
*ML Engineer | Founder, NovalQ | Code Poet*

* ğŸ”— [Portfolio](https://tamaghnatech.in)
* ğŸ™ [GitHub](https://github.com/Tamaghnatech)
* ğŸ§  [W\&B Logs](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm)

---

## ğŸ“£ Final Word

> "A small dataset. A big dream.
> This LLM is tiny in size, but mighty in ambition."
> â€” Tamaghna Nag ğŸ§™â€â™‚ï¸

---
