# ğŸš€ Tiny-LLM: Shakespearean Text Generator

ğŸ­ **Tiny-LLM** is a *local-first*, from-scratch **mini language model** trained entirely on the *Complete Works of Shakespeare*. It generates poetic, Shakespearean, and surreal text using a custom-built **Transformer architecture**.

ğŸ’¡ This repo demonstrates the full **Machine Learning to MLOps** lifecycle: data prep, modeling (LSTM, GRU, Transformer), training, experiment tracking, CLI + Gradio UI, and Hugging Face Spaces deployment.

ğŸŒ **[ğŸ‘‰ Try it LIVE on Hugging Face Spaces](https://huggingface.co/spaces/Tamaghnatech/Tiny-LLM)**

ğŸ“Š **[ğŸ“ˆ See the W&B Dashboard](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm?nw=nwusernagtamaghna)** â€“ Training logs, loss curves, model configs, gradient flow & more.

---

## ğŸ“¦ Download the Trained Model (for Local Usage)

Due to GitHub's size restrictions, the trained Transformer model is hosted externally.

ğŸ“¥ **[Download `transformer_final.pt` from Google Drive](https://drive.google.com/file/d/1HanIwaT0_sILx3-jDXfmmgYqUfHiI_S-/view?usp=sharing)**

Save it to:

```bash
Tiny-LLM/
â””â”€â”€ models/
    â””â”€â”€ transformer_final.pt
```

---

## ğŸ§  End-to-End Pipeline Overview

> A full-stack, production-grade journey from dataset to deployment.

### ğŸ“Š 1. Data Collection & Preparation
- Source: Kaggle Shakespeare Corpus
- Cleaned into character-level dataset â†’ `input.txt` and `input_large.txt`
- Script: [`prepare_dataset.py`](prepare_dataset.py)

### ğŸ§  2. Model Development
- LSTM / GRU (baselines)
- Transformer (final)
  - 8 Layers, 8 Heads, 512 Embedding Dim, Block Size: 256
- Location: [`source/`](source/) directory

### ğŸ‹ï¸â€â™‚ï¸ 3. Training
- Scripts: `trainer_lstm.py`, `trainer_gru.py`, `trainer_transformer.py`
- Tracked using Weights & Biases (`wandb`)
- Optimizer: `AdamW`
- Loss: CrossEntropy
- Checkpoints: `.pt` files saved to `models/`

### ğŸ“ˆ 4. Experiment Tracking (MLOps)
- Dashboard:  
  ğŸ”— [W&B Project](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm?nw=nwusernagtamaghna)

### ğŸ§ª 5. Inference & Serving

#### â¤ CLI Interface
- Script: `generate_text.py`

#### â¤ Gradio Web App
- Script: `app.py`

### ğŸš€ 6. Deployment
- âœ… Gradio Localhost
- âœ… Hugging Face Spaces

---

## ğŸ“‚ Project Structure

```bash
Tiny-LLM/
â”œâ”€â”€ app.py
â”œâ”€â”€ generate_text.py
â”œâ”€â”€ trainer_transformer.py
â”œâ”€â”€ prepare_dataset.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input.txt
â”‚   â”œâ”€â”€ input_large.txt
â”‚   â””â”€â”€ Shakespeare_data.csv
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ transformer_final.pt
â”‚
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ model_transformer.py
â”‚   â”œâ”€â”€ model_lstm.py
â”‚   â”œâ”€â”€ model_gru.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ tokenizer_bpe.py
â”‚
â”œâ”€â”€ logs/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Model Configuration

| Hyperparameter | Value       |
|----------------|-------------|
| Model Type     | Transformer |
| Layers         | 8           |
| Heads          | 8           |
| Embedding Dim  | 512         |
| Block Size     | 256         |
| Tokenizer      | Char-level  |
| Optimizer      | AdamW       |
| Steps Trained  | 100         |

---

## ğŸ’» Run Locally

```bash
git clone https://github.com/Tamaghnatech/Tiny-LLM.git
cd Tiny-LLM

python -m venv venv
source venv/bin/activate   # or venv\Scripts\activate on Windows

pip install -r requirements.txt
```

Download the model and run:

```bash
python app.py
```

Visit [http://localhost:7860](http://localhost:7860)

---

## ğŸ”¥ Gradio Features

- Prompt input box
- Length, Temperature, Top-k sliders
- Light/Dark mode
- Output download
- Prompt history
- Hugging Face embed support

---

## ğŸ§ª Use from Terminal

```bash
python generate_text.py
```

---

## ğŸ› ï¸ Dev Roadmap

- [x] LSTM / GRU baselines
- [x] Transformer model
- [x] Gradio + CLI
- [x] W&B tracking
- [x] Hugging Face deployment
- [ ] Byte Pair Encoding
- [ ] Multilingual toggle
- [ ] Attention viz
- [ ] Docker + PyPI

---

## ğŸ‘¨â€ğŸ’» Author

**Tamaghna Nag**  
Founder of NovalQ | ML Engineer | Code Poet

- ğŸŒ [Portfolio](https://tamaghnatech.in)  
- ğŸ§  [W&B Logs](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm?nw=nwusernagtamaghna)

---

## ğŸª„ Final Word

> â€œA tiny model trained on timeless literature.  
> Proof that small things, when trained right, can be legendary.â€

---

â­ **Fork. Prompt. Publish.**  
ğŸ”— [Launch the App on Hugging Face](https://huggingface.co/spaces/Tamaghnatech/Tiny-LLM)
