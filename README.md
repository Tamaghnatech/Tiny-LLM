# ğŸš€ Tiny-LLM: Shakespearean Text Generator

ğŸ­ **Tiny-LLM** is a *local-first* mini Language Model that generates Shakespearean, poetic, and surreal text using a **Transformer**, trained entirely from scratch on the *Complete Works of Shakespeare*.

ğŸ’» Supports both **Gradio UI** and **Command Line Interface** (CLI). Write prompts, tune temperature, toggle light/dark mode, and download your AI-written verses.

## ğŸŒ Try it Live

[![Hugging Face Spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/Tamaghnatech/Tiny-LLM)

---

## ğŸ“¦ Download the Trained Model (for Local Usage)

The trained `transformer_final.pt` model is not included in the repo due to GitHub size limits.

ğŸ“¥ **[Download the model from Google Drive](https://drive.google.com/file/d/1HanIwaT0_sILx3-jDXfmmgYqUfHiI_S-/view?usp=sharing)**

Save it as:

```bash
Tiny-LLM/
â””â”€â”€ models/
    â””â”€â”€ transformer_final.pt
```

---

## ğŸ§  Tiny-LLM: From Prompt to Poetry

> Built from scratch using PyTorch and love â¤ï¸  
> No Hugging Face pretrained models. No shortcuts. Just math, code, and Shakespeare.

---

## ğŸ“– Project Evolution

### 1. ğŸ§ª Started Small
- Dataset: `input.txt` â€” a small set of Shakespearean lines
- Trained LSTM and GRU models to generate characters

### 2. ğŸ“š Dataset Expansion
- Downloaded the full *Shakespeare Plays Corpus* from Kaggle
- Cleaned and stored as `input_large.txt`

### 3. ğŸ§± Built Transformer from Scratch
- 8 layers, 8 heads, 512 embedding dim
- Block size: 256
- Trained on GPU (100 steps)

### 4. ğŸ“Ÿ Created CLI Tool
- `generate_text.py` for terminal-based text generation

### 5. ğŸ¨ Designed Gradio Web App
- Real-time generation with sliders for Temperature, Top-k, and Length
- Light/Dark mode, session logs, model insights

---

## ğŸ“Š W&B Dashboard

All training experiments were tracked using Weights & Biases:

- ğŸ”— [Project Dashboard](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm)
- ğŸ“ˆ [Transformer Run](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm/runs/9000xl8r)

---

## ğŸ—‚ Project Structure

```bash
Tiny-LLM/
â”œâ”€â”€ app.py
â”œâ”€â”€ generate_text.py
â”œâ”€â”€ trainer_transformer.py
â”œâ”€â”€ prepare_dataset.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input.txt
â”‚   â”œâ”€â”€ input_large.txt
â”‚   â””â”€â”€ Shakespeare_data.csv
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ transformer_final.pt
â”‚
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ model_transformer.py
â”‚   â”œâ”€â”€ model_lstm.py
â”‚   â”œâ”€â”€ model_gru.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ tokenizer_bpe.py
â”‚
â”œâ”€â”€ logs/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Hyperparameters

| Name           | Value       |
|----------------|-------------|
| Model Type     | Transformer |
| Layers         | 8           |
| Heads          | 8           |
| Embedding Size | 512         |
| Block Size     | 256         |
| Tokenizer      | Char-level  |
| Optimizer      | AdamW       |
| Steps Trained  | 100         |

---

## ğŸ’» Run Locally

```bash
git clone https://github.com/Tamaghnatech/Tiny-LLM.git
cd Tiny-LLM

python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

Then, place the model inside `models/` and launch:

```bash
python app.py
```

---

## ğŸ”¥ Gradio Features

- Prompt input box  
- Length slider (50â€“1000)  
- Temperature control (creativity)  
- Top-k sampling  
- Light/Dark toggle  
- Model explanation  
- Prompt history  
- Downloadable output  
- Timeline + Project overview  

---

## ğŸ§ª Use from Terminal

```bash
python generate_text.py
```

Youâ€™ll be asked for:
- Prompt
- Length
- Temperature
- Top-k

---

## ğŸ› ï¸ Dev Roadmap

- [x] LSTM & GRU baselines  
- [x] Train Transformer from scratch  
- [x] Gradio + CLI interfaces  
- [x] Weights & Biases logging  
- [x] Hugging Face Spaces demo  
- [ ] BPE tokenizer  
- [ ] Attention visualization  
- [ ] Language toggle  
- [ ] Docker + PyPI support  
- [ ] Long-context training  

---

## ğŸ‘¨â€ğŸ’» Author

**Tamaghna Nag**  
Founder of NovalQ | ML Engineer | Shakespeare Whisperer

- [Portfolio](https://tamaghnatech.in)  
- [GitHub](https://github.com/Tamaghnatech)  
- [W&B Project](https://wandb.ai/nagtamaghna-oxford-vision-and-sensor-technology/tiny-llm)  

---

## ğŸ’¬ Final Word

> â€œA tiny model built on timeless literature.  
> Proof that even small things, when trained well, can sound divine.â€

---

## ğŸª„ Fork It. Prompt It. Publish It.

â­ Star this repo if you like the work and [play with the app on Hugging Face](https://huggingface.co/spaces/Tamaghnatech/Tiny-LLM)!
