# app.py

from datetime import datetime
import torch
import gradio as gr
from source.model_transformer import TransformerLanguageModel
from source.utils import CharTokenizer
import os

# Load tokenizer and model
with open("data/input_large.txt", "r", encoding="utf-8") as f:
    text_data = f.read()

tokenizer = CharTokenizer(text_data)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = TransformerLanguageModel(
    vocab_size=tokenizer.vocab_size,
    block_size=256,
    embed_dim=512,
    n_layers=8,
    n_heads=8
).to(DEVICE)

model.load_state_dict(torch.load("models/transformer_final.pt", map_location=DEVICE))
model.eval()

# Session log
os.makedirs("logs", exist_ok=True)
session_path = f"logs/session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
with open(session_path, "w", encoding="utf-8") as f:
    f.write("ğŸ§  Tiny LLM Interactive Session Log\n")
    f.write(f"ğŸ“… Started: {datetime.now()}\n")
    f.write("=" * 50 + "\n")

@torch.no_grad()
def generate_text(prompt, length, temperature, top_k, history):
    context = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=DEVICE).unsqueeze(0)
    for _ in range(length):
        input_ids = context[:, -256:]
        logits, _ = model(input_ids)
        logits = logits[:, -1, :] / temperature
        if top_k > 0:
            topk_vals, _ = torch.topk(logits, top_k)
            logits[logits < topk_vals[:, [-1]]] = -float('Inf')
        probs = torch.nn.functional.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        context = torch.cat([context, next_token], dim=1)

    output_text = tokenizer.decode(context.squeeze().tolist())
    timestamp = datetime.now().strftime('%H:%M:%S')
    history.append(f"[{timestamp}] Prompt: {prompt} â†’ {length} chars")
    with open(session_path, "a", encoding="utf-8") as f:
        f.write(f"\n[{timestamp}] Prompt: {prompt}\n{output_text}\n")
    return output_text, history, session_path

# ğŸ”§ Interface setup
description_md = """
## ğŸ¤– Tiny-LLM: Transformer Text Generator
Generate Shakespearean/AI-speak text using a Transformer model trained **from scratch**!

> Type in a prompt, play with creativity, and watch your words evolve!

- ğŸ”¹ Prompt â†’ What should the model start with?
- ğŸ”¢ Length â†’ How much should it write?
- ğŸŒ¡ï¸ Temperature â†’ Lower = logical, Higher = wild ğŸ­
- ğŸ¯ Top-k â†’ Controls randomness (0 = disabled)

âš ï¸ Everything runs **locally** and uses **your GPU** if available.
"""

timeline_md = """
### ğŸ›£ï¸ Project Timeline
| Stage | Description |
|-------|-------------|
| âœ… | Preprocessing, Tokenizer Design |
| âœ… | Transformer Architecture |
| âœ… | 100-Step Training on Custom Text |
| âœ… | CLI Text Generator |
| âœ… | WandB Logging |
| âœ… | Gradio App (You're here!) |
| ğŸ”œ | BPE Tokenizer Upgrade |
| ğŸ”œ | HuggingFace Deployment |
| ğŸ”œ | SHAP / Attention Visualizations |
| ğŸ”œ | Real-time Streaming UI |
"""

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(description_md)
    gr.Markdown(timeline_md)

    with gr.Accordion("ğŸ“Š Model Stats", open=False):
        gr.Markdown(f"""
        - **Layers**: 8  
        - **Embed Dim**: 512  
        - **Heads**: 8  
        - **Block Size**: 256  
        - **Vocab Size**: {tokenizer.vocab_size}  
        - **Device**: `{DEVICE}`  
        """)

    with gr.Row():
        prompt = gr.Textbox(label="ğŸ”¹ Prompt", placeholder="Once upon a time...")
        length = gr.Slider(label="ğŸ”¢ Length", minimum=50, maximum=1000, step=50, value=300)

    with gr.Row():
        temperature = gr.Slider(label="ğŸŒ¡ï¸ Temperature", minimum=0.5, maximum=1.5, step=0.1, value=1.0)
        top_k = gr.Slider(label="ğŸ¯ Top-k Sampling", minimum=0, maximum=100, step=5, value=40)

    btn = gr.Button("ğŸš€ Generate Text")
    output = gr.Textbox(label="ğŸ“ Generated Text", lines=12, max_lines=30)
    history_box = gr.HighlightedText(label="ğŸ•˜ History (Prompts Used)")
    log_file = gr.File(label="ğŸ“¥ Download Full Session Log")

    state = gr.State([])

    btn.click(fn=generate_text, inputs=[prompt, length, temperature, top_k, state],
              outputs=[output, state, log_file])

demo.launch()
